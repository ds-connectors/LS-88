{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Visualizing Text Data\n",
    "\n",
    "\n",
    "**Introduction**\n",
    "...\n",
    "\n",
    "\n",
    "**Outline**\n",
    "...\n",
    "\n",
    "Reference: \n",
    "* http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html\n",
    "* http://sebastianraschka.com/Articles/2014_twitter_wordcloud.html#b-creating-the-word-cloud-b-creating-the-word-cloud\n",
    "* http://www.nltk.org/book/ch03.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install git+git://github.com/amueller/word_cloud.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from urllib.request import urlopen \n",
    "import re                                                                                                                \n",
    "def read_url(url): \n",
    "    return re.sub('\\\\s+', ' ', urlopen(url).read().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from the Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "little_women_url = 'http://www.gutenberg.org/cache/epub/35534/pg35534.txt'\n",
    "little_women_text = read_url(little_women_url)\n",
    "chapters = little_women_text.split('CHAPTER ')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Question, could you make the program read the data from disk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing the first 1000 words of the book\n",
    "little_women_text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "nopunctuation = little_women_text.lower()\n",
    "#remove the punctuation using the character deletion step of translate\n",
    "little_women_text_with_noPun = nopunctuation.translate(string.punctuation)\n",
    "\n",
    "tokens = nltk.word_tokenize(little_women_text_with_noPun)\n",
    "#type(tokens)\n",
    "#len(tokens)\n",
    "\n",
    "tokens[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "count = Counter(filtered)\n",
    "most_common10 = count.most_common(10)\n",
    "most_common10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "count = Counter(tokens)\n",
    "#print count.most_common(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCloud generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for generating word cloud\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(\n",
    "                      font_path='/Users/sebastian/Library/Fonts/CabinSketch-Bold.ttf',\n",
    "                      stopwords=STOPWORDS,\n",
    "                      background_color='black',\n",
    "                      width=1800,\n",
    "                      height=1400\n",
    "                     ).generate(filtered)\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight about the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table().with_columns([\n",
    "    \"Jo\",   np.char.count(chapters, \"Jo\"),\n",
    "    \"Meg\",  np.char.count(chapters, \"Meg\"),\n",
    "    \"Amy\",  np.char.count(chapters, \"Amy\"),\n",
    "    \"Beth\", np.char.count(chapters, \"Beth\"),\n",
    "    \"Laurie\", np.char.count(chapters, \"Laurie\")\n",
    "]).cumsum().plot(overlay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table().with_columns([\n",
    "        \"Characters\", [len(c) for c in chapters],\n",
    "        \"Periods\", np.char.count(chapters, \".\"),\n",
    "    ]).scatter('Periods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
